{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Computer Vision Handwriting Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I originally drafted this computer vision algorithm in MATLAB as part of an assignment for Andrew Ng's Machine Learning course on Coursera. To better practice industry tools in the Python scientific computing stack, I transcribed the code into Python (numpy, scipy) and reformatted it to be run in a Jupyter notebook. Note that this does not include a training/test split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the 3-layer neural network is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 400 #20x20 input images of digits\n",
    "hidden_layer_size = 25 #25 hidden units\n",
    "output_layer_size = 10 #10 labels, from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(training_file='mnist_data.mat'):\n",
    "    \"\"\"\n",
    "    Load training data. Return (inputs, labels).\n",
    "    inputs - numpy array, size (5000,400).\n",
    "    labels - numpy array, size (5000,1).\n",
    "    \n",
    "    This data can be found in Exercise 4 of the Coursera machine learning course and is named ex4data1.mat.\n",
    "    \"\"\"\n",
    "    \n",
    "    training_data = sio.loadmat(training_file)\n",
    "    x = training_data['X']\n",
    "    y = training_data['y']\n",
    "    \"training_data = sio.loadmat(training_file)\"\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, the neural network parameters are pre-initialized, so we load those in as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(weight_file='initial_weights.mat'):\n",
    "    weights = sio.loadmat(weight_file)\n",
    "    theta1 = weights['Theta1']\n",
    "    theta2 = weights['Theta2']\n",
    "    return (theta1, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by implementing the feedforward propagation that returns the cost only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_weights(input_size, output_size):\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.rand(output_size, 1 + input_size) * 2 * epsilon_init - epsilon_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + pow(math.e, -z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function(theta1, theta2, input_layer_size, hidden_layer_size, output_layer_size, x, y, lambda_val=0):\n",
    "    \"\"\"\n",
    "    Below are sizes of the numpy arrays passed as parameters into this function:\n",
    "    theta1: (25, 401)\n",
    "    theta2: (10, 26)\n",
    "    x: (5000, 400)\n",
    "    y: (5000, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #preparing the layers of the neural net\n",
    "    input_layer = np.insert(x, 0, 1, axis=1) #adding a bias column; input_layer dimensions (5000, 401)\n",
    "    hidden_layer = np.dot(input_layer, np.transpose(theta1))\n",
    "    hidden_layer = sigmoid(hidden_layer)\n",
    "    hidden_layer = np.insert(hidden_layer, 0, 1, axis=1) #adding bias; hidden_layer dimensions (5000, 26)\n",
    "    output_layer = np.dot(hidden_layer, np.transpose(theta2)) #5000x10\n",
    "    output_layer = sigmoid(output_layer)\n",
    "    \n",
    "    #forward propagation\n",
    "    cost = 0\n",
    "    #transform each label in y from a number to a (10,1) vector -- binary notation\n",
    "    #indexing starts from 1 and 0 is denoted as 10, e.g. if y[i] is 9, [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "    for index in range(len(x)):\n",
    "        outputs = [0] * output_layer_size\n",
    "        outputs[y[index]-1] = 1\n",
    "    \n",
    "    for k in range(output_layer_size):\n",
    "        cost += -outputs[k] * math.log(output_layer[training_index][k]) - (1 - outputs[k]) * math.log(1 - output_layer[training_index][k])\n",
    "        \n",
    "    #back-propagation\n",
    "    theta1_grad = np.zeros_like(theta1)  #25x401\n",
    "    theta2_grad = np.zeros_like(theta2)  #10x26\n",
    "    for i in range(len(x)):\n",
    "        outputs = np.zeros((1, output_layer_size))  # (1,10)\n",
    "        outputs[0][y[i]-1] = 1\n",
    "\n",
    "        #calculate delta3\n",
    "        delta3 = (output_layer[i] - outputs).T  # (10,1)\n",
    "\n",
    "        #calculate delta2\n",
    "        z2 = np.dot(theta1, input_layer[i:i+1].T)  # (25,401) x (401,1)\n",
    "        z2 = np.insert(z2, 0, 1, axis=0)  # add bias, (26,1)\n",
    "        #below: (26,10) x (10,1) leads to a (26,1) vector\n",
    "        delta2 = np.multiply(np.dot(theta2.T, delta3), sigmoid_gradient(z2))\n",
    "        delta2 = delta2[1:]  #(25,1)\n",
    "\n",
    "        #(25,401) = (25,1) x (1,401)\n",
    "        theta1_grad += np.dot(delta2, input_layer[i:i+1])\n",
    "        #(10,26) = (10,1) x (1,26)\n",
    "        theta2_grad += np.dot(delta3, hidden_layer[i:i+1])\n",
    "    theta1_grad /= len(x)\n",
    "    theta2_grad /= len(x)\n",
    "\n",
    "    return cost, (theta1_grad, theta2_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b2428387f958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-b2428387f958>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inputs, labels, learningrate, iteration)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-b2428387f958>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(inputs, labels, learningrate, iteration)\u001b[0m\n\u001b[1;32m     11\u001b[0m         cost, (theta1_grad, theta2_grad) = nn_cost_function(theta1, theta2,\n\u001b[1;32m     12\u001b[0m             \u001b[0minput_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             inputs, labels)\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtheta1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta1_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtheta2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta2_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-ad999df5a8f9>\u001b[0m in \u001b[0;36mnn_cost_function\u001b[0;34m(theta1, theta2, input_layer_size, hidden_layer_size, output_layer_size, x, y, lambda_val)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutput_layer_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "def gradient_descent(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    '''\n",
    "    @return cost and trained model (weights).\n",
    "    '''\n",
    "    rand_theta1 = random_init_weights(input_layer_size, hidden_layer_size)\n",
    "    rand_theta2 = random_init_weights(hidden_layer_size, output_layer_size)\n",
    "    theta1 = rand_theta1\n",
    "    theta2 = rand_theta2\n",
    "    cost = 0.0\n",
    "    for i in range(iteration):\n",
    "        cost, (theta1_grad, theta2_grad) = nn_cost_function(theta1, theta2,\n",
    "            input_layer_size, hidden_layer_size, output_layer_size,\n",
    "            inputs, labels)\n",
    "        theta1 -= learningrate * theta1_grad\n",
    "        theta2 -= learningrate * theta2_grad\n",
    "        print('Iteration {0} (learning rate {2}, iteration {3}), cost: {1}'.format(i+1, cost, learningrate, iteration))\n",
    "    return cost, (theta1, theta2)\n",
    "\n",
    "\n",
    "def train(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    cost, model = gradient_descent(inputs, labels, learningrate, iteration)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    theta1, theta2 = model\n",
    "    a1 = np.insert(inputs, 0, 1, axis=1)  # add bias, (5000,401)\n",
    "    a2 = np.dot(a1, theta1.T)  # (5000,401) x (401,25)\n",
    "    a2 = sigmoid(a2)\n",
    "    a2 = np.insert(a2, 0, 1, axis=1)  # add bias, (5000,26)\n",
    "    a3 = np.dot(a2, theta2.T)  # (5000,26) x (26,10)\n",
    "    a3 = sigmoid(a3)  # (5000,10)\n",
    "    return [i.argmax()+1 for i in a3]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: There are 10 units which present the digits [1-9, 0]\n",
    "    # (in order) in the output layer.\n",
    "    inputs, labels = load_training_data()\n",
    "\n",
    "    model = train(inputs, labels, learningrate=0.1, iteration=60)\n",
    "\n",
    "    outputs = predict(model, inputs)\n",
    "\n",
    "    correct_prediction = 0\n",
    "    for i, predict in enumerate(outputs):\n",
    "        if predict == labels[i][0]:\n",
    "            correct_prediction += 1\n",
    "    precision = float(correct_prediction) / len(labels)\n",
    "    print('precision: {}'.format(precision))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
