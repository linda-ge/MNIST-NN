{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Computer Vision Handwriting Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I originally drafted this computer vision algorithm in MATLAB as part of an assignment for Andrew Ng's Machine Learning course on Coursera. To better practice industry tools in the Python scientific computing stack, I transcribed the code into Python (numpy, scipy) and reformatted it to be run in a Jupyter notebook. Note that this does not include a training/test split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the 3-layer neural network is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 400 #20x20 input images of digits\n",
    "hidden_layer_size = 25 #25 hidden units\n",
    "output_layer_size = 10 #10 labels, from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(training_file='mnist_data.mat'):\n",
    "    \"\"\"\n",
    "    Load training data. Return (inputs, labels).\n",
    "    inputs - numpy array, size (5000,400).\n",
    "    labels - numpy array, size (5000,1).\n",
    "    \n",
    "    This data can be found in Exercise 4 of the Coursera machine learning course and is named ex4data1.mat.\n",
    "    \"\"\"\n",
    "    \n",
    "    training_data = sio.loadmat(training_file)\n",
    "    x = training_data['X']\n",
    "    y = training_data['y']\n",
    "    \"training_data = sio.loadmat(training_file)\"\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, the neural network parameters are pre-initialized, so we load those in as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(weight_file='initial_weights.mat'):\n",
    "    weights = sio.loadmat(weight_file)\n",
    "    theta1 = weights['Theta1']\n",
    "    theta2 = weights['Theta2']\n",
    "    return (theta1, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by implementing the feedforward propagation that returns the cost only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_weights(input_size, output_size):\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.rand(output_size, 1 + input_size) * 2 * epsilon_init - epsilon_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + pow(math.e, -z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function(theta1, theta2, input_layer_size, hidden_layer_size, output_layer_size, x, y, lambda_val=3):\n",
    "    \"\"\"\n",
    "    Below are sizes of the numpy arrays passed as parameters into this function:\n",
    "    theta1: (25, 401)\n",
    "    theta2: (10, 26)\n",
    "    x: (5000, 400)\n",
    "    y: (5000, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #preparing the layers of the neural net\n",
    "    input_layer = np.insert(x, 0, 1, axis=1) #adding a bias column; input_layer dimensions (5000, 401)\n",
    "    hidden_layer = np.dot(input_layer, np.transpose(theta1))\n",
    "    hidden_layer = sigmoid(hidden_layer)\n",
    "    hidden_layer = np.insert(hidden_layer, 0, 1, axis=1) #adding bias; hidden_layer dimensions (5000, 26)\n",
    "    output_layer = np.dot(hidden_layer, np.transpose(theta2)) #5000x10\n",
    "    output_layer = sigmoid(output_layer)\n",
    "    \n",
    "    #forward propagation\n",
    "    cost = 0\n",
    "    #transform each label in y from a number to a (10,1) vector -- binary notation\n",
    "    #indexing starts from 1 and 0 is denoted as 10, e.g. if y[i] is 9, [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "    for index in range(x.shape[0]):\n",
    "        outputs = [0] * output_layer_size\n",
    "        outputs[int(y[index])-1] = 1\n",
    "        \n",
    "        for k in range(output_layer_size):\n",
    "            cost += -outputs[k] * math.log(output_layer[index][k]) - (1 - outputs[k]) * math.log(1 - output_layer[index][k])\n",
    "        \n",
    "    #back-propagation\n",
    "    theta1_grad = np.zeros_like(theta1)  #25x401\n",
    "    theta2_grad = np.zeros_like(theta2)  #10x26\n",
    "    for i in range(len(x)):\n",
    "        outputs = np.zeros((1, output_layer_size))  # (1,10)\n",
    "        outputs[0][y[i]-1] = 1\n",
    "\n",
    "        #calculate delta3\n",
    "        delta3 = (output_layer[i] - outputs).T  # (10,1)\n",
    "\n",
    "        #calculate delta2\n",
    "        z2 = np.dot(theta1, input_layer[i:i+1].T)  # (25,401) x (401,1)\n",
    "        z2 = np.insert(z2, 0, 1, axis=0)  # add bias, (26,1)\n",
    "        #below: (26,10) x (10,1) leads to a (26,1) vector\n",
    "        delta2 = np.multiply(np.dot(theta2.T, delta3), sigmoid(z2))\n",
    "        delta2 = delta2[1:]  #(25,1)\n",
    "\n",
    "        #(25,401) = (25,1) x (1,401)\n",
    "        theta1_grad += np.dot(delta2, input_layer[i:i+1])\n",
    "        #(10,26) = (10,1) x (1,26)\n",
    "        theta2_grad += np.dot(delta3, hidden_layer[i:i+1])\n",
    "    theta1_grad /= len(x)\n",
    "    theta2_grad /= len(x)\n",
    "\n",
    "    return cost, (theta1_grad, theta2_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    '''\n",
    "    @return cost and trained model (weights).\n",
    "    '''\n",
    "    rand_theta1 = random_init_weights(input_layer_size, hidden_layer_size)\n",
    "    rand_theta2 = random_init_weights(hidden_layer_size, output_layer_size)\n",
    "    theta1 = rand_theta1\n",
    "    theta2 = rand_theta2\n",
    "    cost = 0.0\n",
    "    for i in range(iteration):\n",
    "        cost, (theta1_grad, theta2_grad) = nn_cost_function(theta1, theta2,\n",
    "            input_layer_size, hidden_layer_size, output_layer_size,\n",
    "            inputs, labels)\n",
    "        theta1 -= learningrate * theta1_grad\n",
    "        theta2 -= learningrate * theta2_grad\n",
    "        #print('Iteration {0} (learning rate {2}, iteration {3}), cost: {1}'.format(i+1, cost, learningrate, iteration))\n",
    "    return cost, (theta1, theta2)\n",
    "\n",
    "\n",
    "def train(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    cost, model = gradient_descent(inputs, labels, learningrate, iteration)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    theta1, theta2 = model\n",
    "    a1 = np.insert(inputs, 0, 1, axis=1)  # add bias, (5000,401)\n",
    "    a2 = np.dot(a1, theta1.T)  # (5000,401) x (401,25)\n",
    "    a2 = sigmoid(a2)\n",
    "    a2 = np.insert(a2, 0, 1, axis=1)  # add bias, (5000,26)\n",
    "    a3 = np.dot(a2, theta2.T)  # (5000,26) x (26,10)\n",
    "    a3 = sigmoid(a3)  # (5000,10)\n",
    "    return [i.argmax()+1 for i in a3]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: There are 10 units which present the digits [1-9, 0]\n",
    "    # (in order) in the output layer.\n",
    "    inputs, labels = load_training_data()\n",
    "\n",
    "    model = train(inputs, labels,0.5,5000)\n",
    "\n",
    "    outputs = predict(model, inputs)\n",
    "\n",
    "    correct_prediction = 0\n",
    "    for i, predict in enumerate(outputs):\n",
    "        if predict == labels[i][0]:\n",
    "            correct_prediction += 1\n",
    "    precision = float(correct_prediction) / len(labels)\n",
    "    print('precision: {}'.format(precision))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
